# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('Iris.csv')

# Display basic information about the dataset
print("=" * 50)
print("DATASET OVERVIEW")
print("=" * 50)
print(f"Dataset shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset information:")
print(df.info())
print("\nStatistical summary:")
print(df.describe())
print("\nSpecies distribution:")
print(df['Species'].value_counts())

# Data Preprocessing
print("\n" + "=" * 50)
print("DATA PREPROCESSING")
print("=" * 50)

# Drop the 'Id' column as it's not useful for prediction
df = df.drop('Id', axis=1)

# Check for missing values
print("Missing values in each column:")
print(df.isnull().sum())

# Encode the target variable
label_encoder = LabelEncoder()
df['Species_encoded'] = label_encoder.fit_transform(df['Species'])

# Display the mapping
print("\nSpecies encoding:")
for i, species in enumerate(label_encoder.classes_):
    print(f"{species}: {i}")

# Data Visualization
print("\n" + "=" * 50)
print("DATA VISUALIZATION")
print("=" * 50)

# Set up the plotting style
plt.style.use('seaborn-v0_8')
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Species distribution
df['Species'].value_counts().plot(kind='bar', ax=axes[0,0], color=['skyblue', 'lightcoral', 'lightgreen'])
axes[0,0].set_title('Species Distribution')
axes[0,0].set_xlabel('Species')
axes[0,0].set_ylabel('Count')

# 2. Sepal length vs Sepal width
species_colors = {'Iris-setosa': 'red', 'Iris-versicolor': 'blue', 'Iris-virginica': 'green'}
for species in df['Species'].unique():
    species_data = df[df['Species'] == species]
    axes[0,1].scatter(species_data['SepalLengthCm'], species_data['SepalWidthCm'], 
                     label=species, alpha=0.7)
axes[0,1].set_title('Sepal Length vs Sepal Width')
axes[0,1].set_xlabel('Sepal Length (cm)')
axes[0,1].set_ylabel('Sepal Width (cm)')
axes[0,1].legend()

# 3. Petal length vs Petal width
for species in df['Species'].unique():
    species_data = df[df['Species'] == species]
    axes[1,0].scatter(species_data['PetalLengthCm'], species_data['PetalWidthCm'], 
                     label=species, alpha=0.7)
axes[1,0].set_title('Petal Length vs Petal Width')
axes[1,0].set_xlabel('Petal Length (cm)')
axes[1,0].set_ylabel('Petal Width (cm)')
axes[1,0].legend()

# 4. Box plot for feature distributions
df_melted = pd.melt(df, id_vars=['Species'], 
                   value_vars=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])
sns.boxplot(data=df_melted, x='variable', y='value', hue='Species', ax=axes[1,1])
axes[1,1].set_title('Feature Distributions by Species')
axes[1,1].set_xlabel('Features')
axes[1,1].set_ylabel('Values (cm)')
axes[1,1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = df.drop('Species_encoded', axis=1).select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Heatmap')
plt.show()

# Prepare features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df['Species_encoded']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n" + "=" * 50)
print("MODEL TRAINING")
print("=" * 50)
print(f"Training set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=3),
    'Support Vector Machine': SVC(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

# Train and evaluate models
results = {}

for name, model in models.items():
    # Train the model
    if name in ['Logistic Regression', 'Support Vector Machine']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    
    print(f"\n{name}:")
    print(f"Accuracy: {accuracy:.4f}")

# Find the best model
best_model_name = max(results, key=results.get)
best_model_accuracy = results[best_model_name]

print("\n" + "=" * 50)
print("BEST MODEL")
print("=" * 50)
print(f"Best Model: {best_model_name}")
print(f"Best Accuracy: {best_model_accuracy:.4f}")

# Detailed evaluation of the best model
print("\n" + "=" * 50)
print("DETAILED EVALUATION OF BEST MODEL")
print("=" * 50)

# Retrain the best model
if best_model_name in ['Logistic Regression', 'Support Vector Machine']:
    best_model = models[best_model_name]
    best_model.fit(X_train_scaled, y_train)
    y_pred = best_model.predict(X_test_scaled)
else:
    best_model = models[best_model_name]
    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=label_encoder.classes_, 
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix - Best Model')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature importance (for tree-based models)
if hasattr(best_model, 'feature_importances_'):
    print("\nFeature Importances:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance)
    
    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance, x='importance', y='feature')
    plt.title('Feature Importance - Best Model')
    plt.show()

# Prediction on new data
print("\n" + "=" * 50)
print("PREDICTION ON NEW DATA")
print("=" * 50)

# Sample new data for prediction
new_data = np.array([
    [5.1, 3.5, 1.4, 0.2],  # Should be Iris-setosa
    [6.5, 3.0, 5.8, 2.2],  # Should be Iris-virginica
    [5.9, 3.0, 4.2, 1.5]   # Should be Iris-versicolor
])

if best_model_name in ['Logistic Regression', 'Support Vector Machine']:
    new_data_scaled = scaler.transform(new_data)
    predictions = best_model.predict(new_data_scaled)
else:
    predictions = best_model.predict(new_data)

predicted_species = label_encoder.inverse_transform(predictions)

print("Predictions for new data:")
for i, (data, pred) in enumerate(zip(new_data, predicted_species)):
    print(f"Sample {i+1}: {data} -> {pred}")

# Model comparison visualization
plt.figure(figsize=(12, 6))
models_list = list(results.keys())
accuracies = list(results.values())

bars = plt.bar(models_list, accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'violet'])
plt.title('Model Comparison - Accuracy Scores')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0, 1.1)

# Add accuracy values on top of bars
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{accuracy:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

print("\n" + "=" * 50)
print("SUMMARY")
print("=" * 50)
print("The Iris Flower Classification task has been successfully completed!")
print(f"The best performing model is {best_model_name} with {best_model_accuracy:.4f} accuracy.")
print("The model can reliably classify Iris flowers into three species based on their measurements.")
